[[introduction]]
= Introducing Kudu
:author: Kudu Team
:imagesdir: ./images
:icons: font
:toc: left
:toclevels: 3
:doctype: book
:backend: html5
:sectlinks:
:experimental:

Kudu is a ...

(describe the gap Kudu fills / problem Kudu solves)

== Concepts and Terms

.Columnar Data Store

Kudu is a _columnar data store_. Data is stored in one or more tables, which are
split into segments called _tablets_. All cluster operations are coordinated by
a _master_. Kudu relies on the Raft Consensus Algorithm for many of its internal
operations.

. Kudu Benefits
- Designed to be fast for OLAP workloads.
- Tight integration with Cloudera Impala, making it a good, mutable alternative
  to using HDFS with Parquet.
- Strong but flexible consistency model, allowing you to choose consistency
  requirements on a per-request basis, including the option for strict
  serialized consistency.
- Integration with Apache Hadoop, MapReduce, and Spark
- Integration with Cloudera Manager
- High availability. Tablet Servers and Master use the <<raft>>, which ensures
  availability even if _f_ nodes failing, given _2f+1_ nodes in the cluster.
  Reads can be serviced by read-only follower tablets, even in the event of a
  leader tablet failure.

[[raft]]
.Raft Consensus Algorithm

The link:http://raftconsensus.github.io/[Raft consensus algorithm] provides a
way to elect a _leader_ for a distributed cluster from a pool of potential
leaders, or _candidates_. Other cluster members are _followers_, who are not
candidates or leaders, but always look to the current leader for consensus. Kudu
uses the Raft Consensus Algorithm for the election of masters and leader
tablets, as well as determining the success or failure of a given write
operation.

.Table A _table_ is where your data is stored in Kudu. A table has a schema and
a totally ordered primary key. A table is split into segments called tablets, by
primary key.

.Tablet A _tablet_ is a contiguous segment of a table. A given tablet is
replicated on multiple tablet servers, and one of these replicas is considered
the leader tablet. Any replica can service reads, and writes require consensus
among the set of tablet servers serving the tablet.

.Tablet Server A _tablet server_ stores and serves tablets to clients. For a
given tablet, one tablet server serves the lead tablet, and the others serve
follower replicas of that tablet. Only leaders service write requests, while
leaders or followers each service read requests. Leaders are elected using
<<raft>>. One tablet server can serve multiple tablets, and one tablet can be served
by multiple tablet servers.

.Master The _master_ keeps track of all the tablets, tablet servers, the
<<catalog_table>>, and other metadata related to the cluster. At a given point
in time, there can only be one acting master (the leader). If the current leader
disappears, a new master is elected using <<raft>>.

The master also coordinates metadata operations for clients. For example, when
creating a new table, the client internally sends an RPC to the master. The
master writes the metadata for the new table into the catalog table, and
coordinates the process of creating tablets on the tablet servers.

All the master's data is stored in a tablet, which can be replicated to all the
other candidate masters.

Tablet servers heartbeat to the master at a set interval (the default is once
per second).

[[catalog_table]]
.Catalog Table

The _catalog table_ is the central location for
metadata of Kudu. It stores information about tables and tablets. The catalog
table is accessible to clients via the master, using the client API.

Tables:: table schemas, locations, and states

Tablets:: the list of existing tablets, which tablet servers have replicas of
each tablet, the tablet's current state, and start and end keys.

.Logical Replication

Kudu replicates operations, not on-disk data. This is referred to as _logical
replication_, as opposed to _physical replication_. Physical operations, such as
compaction, do not need to transmit the data over the network. This results in a
substantial reduction in network traffic for heavy write scenarios.

== Architectural Overview
INSERT DIAGRAM HERE

== Typical Use Cases
.First Use Case
The first use case

.Second Use Case
The second use case

.Third Use Case
The third use case

== Next Steps
- <<quickstart>>
- <<installation>>
